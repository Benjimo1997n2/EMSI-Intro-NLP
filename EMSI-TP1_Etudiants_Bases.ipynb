{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 1/ Les regex dans le NLP\n",
        "> Question 1:\n",
        "Ecrivez une regex pour trouver les dates dans la phares suivante: \"Paul est né le 12-05-1982 et Marie le 03/04/1990. Ils se sont rencontrés le 23 Septembre 2010.\""
      ],
      "metadata": {
        "id": "2_VaZyLldroo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "texte = \"\"\"\n",
        "Paul est né le 12-05-1982 et Marie le 03/04/1990. Ils se sont rencontrés le 23 Septembre 2010.\n",
        "\"\"\"\n",
        "\n",
        "# Regex pour identifier les dates\n",
        "pattern = #TODO\n",
        "\n",
        "# Recherche des dates dans le texte\n",
        "# TODO\n",
        "\n",
        "print(\"Dates trouvées :\", dates)\n",
        "\n",
        "# Résultat à obtenir: Dates trouvées : ['12-05-1982', '03/04/1990', '23 Septembre 2010']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7MGGrnUYdq8X",
        "outputId": "f95c490e-200a-4018-c49e-23eea5dea71e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dates trouvées : ['12-05-1982', '03/04/1990', '23 Septembre 2010']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Explication:\n",
        "\n",
        "Le code utilise une expression régulière pour identifier les dates sous différents formats (par exemple, \"12-05-1982\", \"03/04/1990\", \"23 Septembre 2010\").\n",
        "re.findall() est utilisé pour trouver toutes les occurrences qui correspondent au pattern dans le texte.\n",
        "\n",
        "## Limites des Regex en NLP\n",
        "Les regex sont puissants pour des motifs bien définis et structurés, mais ils ont des limites, notamment :\n",
        "\n",
        "- Manque de Contexte: Les regex ne comprennent pas le contexte. Par exemple, ils ne peuvent pas distinguer entre des dates dans un contexte historique et des dates mentionnées comme exemples.\n",
        "- Complexité avec des Patterns Variables: Pour des structures de texte plus complexes ou moins standardisées, l'écriture de regex devient très compliquée.\n",
        "- Incapacité à Interpréter: Les regex ne peuvent pas interpréter la signification des mots ou des phrases. Par exemple, ils ne peuvent pas distinguer \"Paul est né le 12-05-1982\" de \"La réunion est prévue pour le 12-05-1982\"."
      ],
      "metadata": {
        "id": "HgyNyHvOd47L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "texte = \"\"\"\n",
        "Ce dimanche 19 novembre 2023 (19-11-23) marque une date historique pour T1, qui s'impose majestueusement avec un score de 3-0 contre Weibo Gaming.\n",
        "Faker était en 06-01-09.\n",
        "\"\"\"\n",
        "\n",
        "# Regex pour identifier les dates\n",
        "pattern = # TODO\n",
        "\n",
        "# Recherche des dates dans le texte\n",
        "# TODO\n",
        "\n",
        "print(\"Dates trouvées :\", dates)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CtOp4cOvdx97",
        "outputId": "5b9660ab-7404-40ef-ebb0-86fccde707e0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dates trouvées : ['19 novembre 2023']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Qu'observez vous?"
      ],
      "metadata": {
        "id": "bxk6HaFHhDKe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2/ ELIZA\n",
        "Pour créer un chatbot simple similaire à ELIZA basé sur les regex et les exemples fournis, nous allons écrire un script Python qui utilise des expressions régulières pour reconnaître des motifs spécifiques dans les entrées de l'utilisateur et y répondre de manière appropriée. Nous allons utiliser les patrons de réponse que vous avez fournis pour définir les règles du chatbot\n",
        "\n",
        "> Question 2:\n",
        "Faites que ELIZA puisse répondre aux messages suivants:\n",
        "- I am sad today.\n",
        "- It feels like everyone is always ignoring me.\n",
        "- My boyfriend is always late.\n",
        "- I think my father doesn't understand me.\n",
        "- I need more help.\n"
      ],
      "metadata": {
        "id": "dzygN3ncmINd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def eliza_chatbot(user_input):\n",
        "    # TODO: Implementez vos règles pour identifier les patterns\n",
        "    # Exemple si \"always\" est detecté, vous pouvez retourner: \"CAN YOU THINK OF A SPECIFIC EXAMPLE\"\n",
        "\n",
        "    return \"\"\n",
        "\n",
        "# Exemple d'utilisation\n",
        "user_input = input(\"You: \")\n",
        "print(\"ELIZA: \" + eliza_chatbot(user_input))\n",
        "\n",
        "# Testez:\n",
        "# I am sad today.\n",
        "# It feels like everyone is always ignoring me.\n",
        "# My boyfriend is always late.\n",
        "# I think my father doesn't understand me.\n",
        "# I need more help.\n",
        "# My sister nerver supports me."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I4xz2Xr7eXZk",
        "outputId": "8920bc72-d16b-45fb-f9a8-d61d16780b16"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "You: My sister nerver supports me.\n",
            "ELIZA: I AM NOT SURE I UNDERSTAND WHAT YOU ARE SAYING\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3/ Concepts Clés en Traitement Automatique du Langage Naturel\n",
        "## 1. Tokenisation\n",
        "La tokenisation est le processus de découpage d'un texte en morceaux, appelés tokens. Ces tokens sont souvent des mots, mais peuvent aussi inclure des ponctuations et d'autres éléments. La tokenisation est une étape fondamentale en TALN car elle permet de préparer le texte pour des analyses plus approfondies."
      ],
      "metadata": {
        "id": "t4EN2ZhUDzEX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q nltk\n",
        "# Ici l'option -q signifie une installation en mode \"silencieux\", c'est-à-dire sans afficher de messages à l'utilisateur pendant le processus d'installation."
      ],
      "metadata": {
        "id": "XJ9gapNSEOpg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "On importe nltk et download les données pour la **tokenisation** et **POS tagging** (fonctions grammaticales des mots)"
      ],
      "metadata": {
        "id": "qvjNBuoUEZFh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "import nltk\n",
        "# Données pour la Tokenisation\n",
        "nltk.download('punkt')\n",
        "# Données pour le pos_tagging\n",
        "nltk.download('averaged_perceptron_tagger')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TvNm3uFUERoY",
        "outputId": "f840f49c-6736-4abb-efcb-7b17174f7d14"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "> Question 3:\n",
        "Essayer de tokeniser une phrase."
      ],
      "metadata": {
        "id": "1LfHvTyf8lyB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "texte = \"The quick brown foxes were jumping over the lazy dogs. #animals\"\n",
        "# TODO: Print les tokens de la variable texte\n",
        "# Doc: https://www.nltk.org/api/nltk.tokenize.word_tokenize.html\n",
        "# Attendu: ['The', 'quick', 'brown', 'foxes', 'were', 'jumping', 'over', 'the', 'lazy', 'dogs', '.', '#', 'animals']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uxrm3LTWjZmh",
        "outputId": "1fafa560-ea28-4cc3-edfc-2daf490a87e8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['The', 'quick', 'brown', 'foxes', 'were', 'jumping', 'over', 'the', 'lazy', 'dogs', '.', '#', 'animals']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Certains tokenizers sont plus appropriés que d'autres selon la situation, dans l'exemple précédent il faudrais interpreter le #AI en un seul bloc plutot que # et AI.\n",
        "> Question 4:\n",
        "Utilisez le *TweetTokenizer* de nltk pour voir la différence."
      ],
      "metadata": {
        "id": "pD1b468BFnch"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import TweetTokenizer\n",
        "# TODO\n",
        "print(tweet_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pk_dFN3qD9V8",
        "outputId": "b8414a0d-5fc4-4ed2-e60c-62af4893b58c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['The', 'quick', 'brown', 'foxes', 'were', 'jumping', 'over', 'the', 'lazy', 'dogs', '.', '#animals']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. POS-tagging (Part-of-Speech Tagging)\n",
        "Le POS-tagging est le processus d'assignation de tags de parties du discours (comme nom, verbe, adjectif, etc.) à chaque token d'un texte. Cela est utile pour comprendre la structure grammaticale d'une phrase et pour d'autres analyses syntaxiques."
      ],
      "metadata": {
        "id": "QpbVnA1hG0PS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> Question 5:\n",
        "Explorez les Part-of-speech d'une phrase."
      ],
      "metadata": {
        "id": "tmJnDVJZ9j26"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk import pos_tag\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "texte = \"The quick brown foxes were jumping over the lazy dogs.\"\n",
        "# TODO: Effectuer le POS-tagging de texte\n",
        "# Tips: Utiliser pos_tag de nltk\n",
        "print(pos_tags)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zD6DEX5iIRE5",
        "outputId": "8cfe0701-a2e6-48b3-82ca-84cb51c0d7e9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('The', 'DT'), ('quick', 'JJ'), ('brown', 'NN'), ('foxes', 'NNS'), ('were', 'VBD'), ('jumping', 'VBG'), ('over', 'IN'), ('the', 'DT'), ('lazy', 'JJ'), ('dogs', 'NNS'), ('.', '.')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "La plupart des mots sont correctement taggés selon leur partie du discours. Cependant, il y a une petite erreur avec \"brown\" étant taggé comme un nom plutôt qu'un adjectif. Cela met en évidence un aspect important des systèmes de POS-tagging : bien qu'ils soient généralement précis, ils ne sont pas infaillibles et peuvent parfois mal interpréter le rôle grammatical de certains mots, en particulier dans des contextes complexes ou ambigus."
      ],
      "metadata": {
        "id": "kvA5CySDJHD7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Stemming\n",
        "Le stemming est un processus de réduction des mots à leur racine ou 'stem'. Cela implique généralement l'élimination des suffixes des mots, ce qui peut parfois mener à des formes qui ne sont pas des mots réels. Le stemming est souvent plus rapide que la lemmatisation, mais moins précis."
      ],
      "metadata": {
        "id": "PYMKwiKwGxk9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> Question 6:\n",
        "Explorez la racinisation (Porter)."
      ],
      "metadata": {
        "id": "AsoJnmyZ-mEi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "texte = \"The quick brown foxes were jumping over the lazy dogs.\"\n",
        "\n",
        "# TODO: Stem de la variable texte\n",
        "\n",
        "print(stems)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RMI50OmIHXIW",
        "outputId": "5f12bf38-e182-4923-d683-3a7a42201b40"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['the', 'quick', 'brown', 'fox', 'were', 'jump', 'over', 'the', 'lazi', 'dog', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Voici quelques commentaires sur les stems obtenus :\n",
        "\n",
        "- 'the' → 'the': Les mots fonctionnels comme les articles ne sont généralement pas modifiés par le stemming.\n",
        "\n",
        "- 'quick' → 'quick': Les adjectifs simples comme \"quick\" ne sont souvent pas modifiés, car ils n'ont pas de suffixes complexes.\n",
        "\n",
        "- 'brown' → 'brown': Semblable à \"quick\", \"brown\" reste inchangé car le stemming n'altère généralement pas les adjectifs simples.\n",
        "\n",
        "- 'foxes' → 'fox': Le mot \"foxes\" est correctement réduit à sa forme de base \"fox\". C'est un bon exemple de la façon dont le stemming peut traiter efficacement les formes plurielles.\n",
        "\n",
        "- 'were' → 'were': Les formes verbales comme \"were\" ne sont souvent pas bien traitées par les algorithmes de stemming simples, car leur transformation nécessiterait une compréhension plus contextuelle.\n",
        "\n",
        "- 'jumping' → 'jump': Le participe présent \"jumping\" est correctement réduit à la forme de base du verbe \"jump\".\n",
        "\n",
        "- 'over' → 'over': Comme \"the\", \"over\" est un mot fonctionnel et reste généralement inchangé.\n",
        "\n",
        "- 'lazy' → 'lazi': Ici, nous voyons une limitation du stemming. Le mot \"lazy\" est réduit à \"lazi\", ce qui n'est pas un mot valide. Cela illustre comment le stemming peut parfois créer des formes de mots non standard.\n",
        "\n",
        "- 'dogs' → 'dog': Comme pour \"foxes\", \"dogs\" est correctement réduit à sa forme singulière \"dog\".\n",
        "\n",
        "- '.' → '.': Les signes de ponctuation restent inchangés."
      ],
      "metadata": {
        "id": "2l64Dt8fKkLK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.Lemmatisation\n",
        "La lemmatisation est un processus de réduction des mots à leur forme de base ou à leur 'lemme'. Contrairement au stemming, la lemmatisation tient compte du contexte et convertit le mot en sa forme lexicale significative. Cela est particulièrement utile pour ramener les mots conjugués à leur forme de base."
      ],
      "metadata": {
        "id": "kFvQ_ID9GsVD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Rm7enFKHKR-",
        "outputId": "3144a983-6ae3-4f40-8ac2-d7cbba1887af"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "On telecharge la base de donnée lexicale Wordnet pour:\n",
        "- Base de Données Lexicale: WordNet est une grande base de données lexicale de l'anglais. Elle regroupe les mots en ensembles de synonymes appelés synsets, et fournit des définitions brèves et des relations sémantiques entre ces synsets. Pour la lemmatisation, WordNet est utilisé pour retrouver le 'lemme', ou la forme de base d'un mot.\n",
        "\n",
        "- Précision de Lemmatisation: Contrairement au stemming, qui coupe simplement les extrémités des mots pour arriver à une forme de base, la lemmatisation utilise une approche plus sophistiquée. Elle implique de comprendre le contexte dans lequel un mot est utilisé pour le ramener à sa forme canonique. Par exemple, le mot \"better\" a pour lemme \"good\" en fonction du contexte. Cette compréhension du contexte et cette relation sémantique sont fournies par WordNet.\n",
        "\n",
        "WordNet n'est pas un modèle entraîné avec du deep learning. WordNet est une base de données lexicale manuellement organisée, et non un modèle d'apprentissage automatique."
      ],
      "metadata": {
        "id": "w2-IL5L9JvsS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> Question 7:\n",
        "Explorez la Lemmatisation et la différence avec le Stemming"
      ],
      "metadata": {
        "id": "kQ9ixoyV_FnC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "texte = \"The quick brown foxes were jumping over the lazy dogs.\"\n",
        "\n",
        "# TODO: Lemmes de la variable texte\n",
        "\n",
        "print(lemmes)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MITy0qDuHEcB",
        "outputId": "c60dd11f-13cf-4f3e-d7c1-15cff3b6f3da"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['The', 'quick', 'brown', 'fox', 'were', 'jumping', 'over', 'the', 'lazy', 'dog', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- \"foxes\" → \"fox\": C'est un exemple typique de lemmatisation. Le mot \"foxes\" est la forme plurielle de \"fox\", et le lemmatiseur a correctement identifié et réduit ce mot à sa forme de base, \"fox\".\n",
        "\n",
        "- \"were\" → \"were\": Ici, le lemmatiseur n'a pas modifié \"were\". C'est parce que le lemmatiseur de NLTK par défaut est principalement conçu pour les noms. Pour lemmatiser correctement les verbes, il faut spécifier le type de mot (partie du discours) lors de la lemmatisation.\n",
        "\n",
        "- \"jumping\" → \"jumping\": Comme \"were\", \"jumping\" n'a pas été réduit à sa forme de base \"jump\". Cela est dû au fait que le lemmatiseur de NLTK sans indication de la partie du discours ne parvient pas toujours à réduire correctement les formes verbales.\n",
        "\n",
        "- \"dogs\" → \"dog\": Tout comme \"foxes\", \"dogs\" est un autre exemple où la lemmatisation a fonctionné correctement. \"Dogs\" est le pluriel de \"dog\", et le lemmatiseur l'a correctement réduit à sa forme singulière."
      ],
      "metadata": {
        "id": "cLWoMV4OH4n2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pour améliorer la lemmatisation des verbes, il est recommandé d'utiliser le POS-tagging pour identifier les parties du discours de chaque mot et de les transmettre au lemmatiseur. Cela permet une lemmatisation plus précise, en particulier pour les verbes. Voici comment vous pourriez le faire :"
      ],
      "metadata": {
        "id": "mwHC9WAaKtnE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> Question 8: Améliorez la Lemmatisation avec les POS-tags"
      ],
      "metadata": {
        "id": "3hbudhHpCrDd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk import pos_tag\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import wordnet\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "def get_wordnet_pos(word):\n",
        "    \"\"\"Map POS tag to first character lemmatize() accepts\"\"\"\n",
        "    tag = pos_tag([word])[0][1][0].upper()\n",
        "    tag_dict = {\"J\": wordnet.ADJ,\n",
        "                \"N\": wordnet.NOUN,\n",
        "                \"V\": wordnet.VERB,\n",
        "                \"R\": wordnet.ADV}\n",
        "\n",
        "    return tag_dict.get(tag, wordnet.NOUN)\n",
        "\n",
        "texte = \"The quick brown foxes were jumping over the lazy dogs.\"\n",
        "\n",
        "# TODO: Lemmes de la variable texte amélioré avec le POS-tagging (fonction au dessus)\n",
        "\n",
        "print(lemmatized)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2SyPf9IjGOG_",
        "outputId": "c606c65e-32bb-4cbc-c852-1103ba6e04e7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['The', 'quick', 'brown', 'fox', 'be', 'jumping', 'over', 'the', 'lazy', 'dog', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 'were' → 'be': Ici, \"were\" est correctement identifié comme une forme du verbe \"to be\" et est ramené à sa forme de base \"be\".\n",
        "- 'jumping' reste 'jumping': Dans ce cas, bien que le mot soit un participe présent, il est conservé sous cette forme car c'est la forme de base pour un verbe avec ce POS-tag."
      ],
      "metadata": {
        "id": "QDEIK9ApMayR"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YjC4Hv2fKwBK"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}